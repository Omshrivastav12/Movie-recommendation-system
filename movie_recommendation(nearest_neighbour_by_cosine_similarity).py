# -*- coding: utf-8 -*-
"""Movie_recommendation(Nearest_neighbour_by_cosine_similarity).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10qqjBSdwNoBFAyW3HNGR4hwmS1V-Y461
"""

kimport pandas as pd
import numpy as np

movies_d=pd.read_csv('/content/tmdb_5000_movies.csv') #loading dataset into notebook

movies_d.head(1)

len(movies_d.columns)

import csv #to handel error of {Error tokenizing data. C error: EOF inside string starting at row 519 } EOF->End of file
with open('/content/tmdb_5000_credits.csv', 'r') as file:
  column1=[];column2=[];column3=[];column4=[]
  reader = csv.reader(file);next(reader, None)
  for i, row in enumerate(reader):
    column1.append(row[0])  # Replace 0 with the index of the column you want
    column2.append(row[1])
    column3.append(row[2])
    column4.append(row[3])

Credit_d=pd.DataFrame({
    "movie_id":column1,
    "title":column2,
    "cast":column3,
    "crew":column4
})

Credit_d.head()

Credit_d['crew']

final_d=pd.merge(Credit_d,movies_d,on='title') #merge used to join two dataframes having same columns ---> while concat used to join two data frames along row(axis=0),or column(axis=1) no need same columns

final_d.shape



final_d.info()

#required columns
#genres,id,keywords,title,overview,cast,crew

final_d=final_d[['title','genres','id','keywords','overview','cast','crew']]

final_d.head(1)

#first step always check  for null and duplicate values
final_d.isnull().sum() #very less null values wee can drop it

final_d.dropna(inplace=True)

final_d.isnull().sum()

final_d.info()

# final_d.duplicated().sum()

final_d['genres'][0]

import ast #module to convert string of list into list
#to extract movies types(eg action,emotional,etc)
def extractor(object):
  empty=[]
  for i in ast.literal_eval(object):
    empty.append(i['name'])
  return empty

final_d['genres']=final_d['genres'].apply(extractor)

final_d['keywords']=final_d['keywords'].apply(extractor)

final_d.head(1)

def extractor(object):
  empty=[];cnt=0
  for i in ast.literal_eval(object):
    if cnt<3:#taking just top 3 names
      empty.append(i['name'])
      cnt+=1
  return empty

final_d['cast']=final_d['cast'].apply(extractor)

final_d.head()

final_d['crew'][0]

def extractor(object):
  empty=[]
  for i in ast.literal_eval(object):
    if i['job']=='Director':#taking name of director
      empty.append(i['name'])
  return empty

final_d['crew']=final_d['crew'].apply(extractor)

final_d['overview']=final_d['overview'].apply(lambda x:x.split())#converting string into list of words

final_d.head()

final_d['genres']=final_d['genres'].apply(lambda x:[i.replace(' ','') for i in x])
final_d['keywords']=final_d['keywords'].apply(lambda x:[i.replace(' ','') for i in x])
final_d['cast']=final_d['cast'].apply(lambda x:[i.replace(' ','') for i in x])
final_d['crew']=final_d['crew'].apply(lambda x:[i.replace(' ','') for i in x])

final_d.head()

final_d['tags']=final_d['genres']+final_d['keywords']+final_d['overview']+final_d['cast']+final_d['crew'] # converting all information into single paragraph

final_d['tags'][0]

new_df=final_d[['title','id','tags']]

new_df.head()

new_df['tags']=new_df['tags'].apply(lambda x:" ".join(x))#removing space from words.

new_df.head()

new_df['tags']=new_df['tags'].apply(lambda x:x.lower())#converting all words into lowercase

new_df['tags'][0]#final tag content

new_df = new_df[['id', 'title', 'tags']]

new_df.head()

import nltk #stemming will convert ['love','loved','loving'] into [love,love,love]
from nltk.stem.porter import PorterStemmer#importing stemming module
ps=PorterStemmer()

def stemm(text):
  l=[]
  for i in text.split():
    l.append(ps.stem(i))
  return ' '.join(l)

new_df['tags']=new_df['tags'].apply(stemm)

from sklearn.feature_extraction.text import CountVectorizer #importing module to convert all imformation into vector so that closer vector will get recommended

cv=CountVectorizer(max_features=5000,stop_words='english')#creating object of required arguments

new_df['tags'][0]

vect_feat=cv.fit_transform(new_df['tags']).toarray()#converting it to numpy array

vect_feat.shape

len((cv.get_feature_names_out()))

print(ps.stem('loved'));print(ps.stem('loving')) #stem examples

"""Now for recommendation generally we use eucledian distance but in higher dimensions eucledian distance is not good measure so  we use cos angle between two vectors instead

"""

from sklearn.metrics.pairwise import cosine_similarity #similarity is just inverse of distance

feat_similarity=cosine_similarity(vect_feat)

feat_similarity.shape#we are calculating distance of each and every movie with each and every movie so total 4806*4806 distances

sorted_dist=sorted(list(enumerate(feat_similarity[0])),reverse=True,key=lambda x:x[1])[2]
list(new_df.loc[sorted_dist[0]])[1]

def recommend(movie):
  movie_index=new_df[new_df['title']==movie].index[0]
  Distances= feat_similarity[movie_index]
  movies_list=sorted(list(enumerate(Distances)),reverse=True,key=lambda x:x[1])[1:6]
  for i in movies_list:
    print(list(new_df.loc[i[0]])[1])

print("The recommended movies are:->>>")
recommend("Independence Day")